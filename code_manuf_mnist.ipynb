{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:59:33.615120Z",
     "start_time": "2019-11-15T17:59:21.301690Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# gestion des matrices, maths \n",
    "import numpy as np\n",
    "\n",
    "# gestion de jeux de données\n",
    "import pandas as pd\n",
    "\n",
    "# affichage / création de graphes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Outil d'évaluation de modèle\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# keras = librarie de Deep Learning / construction de réseaux de neurones\n",
    "import keras \n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:59:50.741770Z",
     "start_time": "2019-11-15T17:59:33.628559Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"data.csv\"\n",
    "\n",
    "#chargement des données depuis notre fichier csv\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "#affichage des premiere ligne de notre dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude exploratoire des donnée  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T18:06:18.687653Z",
     "start_time": "2019-11-15T18:06:18.669495Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_len = len(dataset)\n",
    "print(\"nombre de donnée : \", dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_value_plot = 10\n",
    "\n",
    "# Créer un object pour gérer l'affichage de nos images\n",
    "fig1, axes = plt.subplots(10, nb_value_plot)\n",
    "\n",
    "for i in range(10): # 10 nombres\n",
    "    # Sélectionne seulement les lignes du dataset correspondant au chiffre \"i\".\n",
    "    # sample = récupère aléatoirement \"nb_value_plot\" lignes parmi la sélection\n",
    "    # values[:,1:] = récupère les valeurs à partir de la 2ème. La première étant le label\n",
    "    sample = dataset[dataset['label'] == i].sample(nb_value_plot).values[:,1:]\n",
    "    num_plot=0\n",
    "    for img in sample:\n",
    "        # img correspond à un tableau de format (784,). Pour l'afficher en tant qu'image on le reshape en 28*28px\n",
    "        image = img.reshape((28,28))\n",
    "        axes[i, num_plot].imshow(image)\n",
    "        axes[i, num_plot].axis('off')\n",
    "        num_plot+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T17:33:31.153355Z",
     "start_time": "2019-10-09T17:33:30.826035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution des classes (des chiffres différents)\n",
    "sns.countplot(dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print la valeur minimal et la valeur maximal existante dans la première image du dataset\n",
    "print((min(dataset.values[:,1:][1]), max(dataset.values[:,1:][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum().sum()\n",
    "\n",
    "# il y a 0 valeurs nulles dans le dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séparation des jeux de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T18:06:08.519355Z",
     "start_time": "2019-11-15T18:06:02.593018Z"
    }
   },
   "outputs": [],
   "source": [
    "# On donne un ordre aléatoire aux données pour s'assurer qu'elle ne sont pas rangé par classes\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "# On défini les proportions de chaque set\n",
    "test_portion = 0.1\n",
    "validation_portion = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T18:06:26.247885Z",
     "start_time": "2019-11-15T18:06:26.161754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calcul le nombre d'images par set\n",
    "nb_test_data = int(0.1 * dataset_len)\n",
    "nb_validation_data = int(0.2 * dataset_len)\n",
    "nb_train_data = dataset_len - nb_validation_data - nb_test_data\n",
    "\n",
    "print(\"données de training : \", nb_train_data)\n",
    "print(\"données de validation : \", nb_validation_data)\n",
    "print(\"données de test : \", nb_test_data)\n",
    "\n",
    "\n",
    "# Sépare les données\n",
    "dataset_train = dataset[:nb_train_data]\n",
    "dataset_val = dataset[nb_train_data:nb_train_data+nb_validation_data]\n",
    "dataset_test = dataset[-nb_test_data:]\n",
    "\n",
    "print(\"\\n\\nOn vérifie que ce sont bien les mêmes nombres\")\n",
    "print(\"données de training : \", len(dataset_train))\n",
    "print(\"données de validation : \", len(dataset_val))\n",
    "print(\"données de test : \", len(dataset_test))\n",
    "\n",
    "#séparation du label (Y) des données (X)\n",
    "X_train = dataset_train.values[:,1:]\n",
    "Y_train = dataset_train['label'].values\n",
    "\n",
    "X_val = dataset_val.values[:,1:]\n",
    "Y_val = dataset_val['label'].values\n",
    "\n",
    "X_test = dataset_test.values[:,1:]\n",
    "Y_test = dataset_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T18:06:36.484465Z",
     "start_time": "2019-11-15T18:06:34.195216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Les modèles que l'on va utiliser convergent plus facilement si les inputs sont entre 0 et 1 plutôt que 0 à 255\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /=255\n",
    "X_test /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T18:06:36.648365Z",
     "start_time": "2019-11-15T18:06:36.512402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Il faut aussi modifié le \"Y\" passant d'un chiffre à un array de 0 et 1 : one hot encoding\n",
    "\n",
    "print(\"avant :\", Y_train[0])\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train, 10)\n",
    "Y_val = keras.utils.to_categorical(Y_val, 10)\n",
    "Y_test = keras.utils.to_categorical(Y_test, 10)\n",
    "\n",
    "print(\"après : \", Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation d'un modèle simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vous pouvez jouer sur ces paramètres :\n",
    "- nombre de couches\n",
    "- type de couches\n",
    "- batch\n",
    "- epochs\n",
    "- type d'activation\n",
    "- nombre d'activation (neurones)\n",
    "\n",
    "Voici la documentation : https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T18:16:21.796650Z",
     "start_time": "2019-11-15T18:16:21.695787Z"
    }
   },
   "outputs": [],
   "source": [
    "# créer un object \"model\" de type \"Sequential\"\n",
    "model = Sequential()\n",
    "\n",
    "# Ajoute une couche de type \"Dense\" de 1 neurone. \n",
    "# L'activation utilisé pour cette couche est de type \"sigmoid\".\n",
    "# Etant la première couche du modèle il faut aussi lui préciser la \"shape\" qu'elle va recevoir en input.\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', name = \"Hidden_Layer_1\", input_shape = (784,)))\n",
    "\n",
    "# Nous utilisons softmax pour choisir le chiffre \"gagnant\" et connaître sa probabilité\n",
    "# La somme des 10 chiffres en sortie du softmax = 1. On à donc une probabilité pour chaque chiffre.\n",
    "model.add(Dense(10, activation='softmax', name = \"Output_Layer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T18:16:22.648254Z",
     "start_time": "2019-11-15T18:16:22.437417Z"
    }
   },
   "outputs": [],
   "source": [
    "# On \"compile\" ce qui configure le model et le prépare à l'entrainement. Nous devons choisir : \n",
    "#  - l'optimizer : fait le lien entre la loss et la mise à jour des poids du modèle\n",
    "#  - loss : comment on calcul l'erreur. Ici \"categorical_crossentropy\" car nous avons 10 catégories différentes à classer\n",
    "#  - metrics : ce qui sera affiché lors de l'entraînement\n",
    "\n",
    "model.compile(optimizer = \"sgd\" , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de certains paramètres importants pour l'entraînement\n",
    "\n",
    "# float entre 0 et 1, la majorité du temps entre 0.001 et 0.1.\n",
    "# Défini la vitesse de modification des poids du réseaux de neurones\n",
    "learning_rate = 0.1 \n",
    "\n",
    "# Nombre de fois que le modèle va voir toute les données d'apprentissage\n",
    "training_epochs = 2\n",
    "\n",
    "# Nombre de données envoyées au modèle lors de l'entraînement avant la backpropagation\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# l'entraînement est déclenché avec l'appel de la fonction \"fit\".\n",
    "# on lui envoie les données, les réponse (Y_train) ainsi que les variables choisies au dessus\n",
    "# verbose : niveau d'information qui sera écrit pendant l'entraînement\n",
    "# Pendant l'entraînement on pourra aussi voir si le modèle over-fit ou pas avec la validation_data\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                     batch_size = batch_size,\n",
    "                     epochs = training_epochs,\n",
    "                     verbose = 1,\n",
    "                     validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ici on affiche history (ce qui est retourné par model.fit) .history['loss'] et 'val_loss' \n",
    "# pour avoir une vision plus graphique de leur évolution\n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "# Pareil pour l'accuracy\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)\n",
    "\n",
    "# ces plots nous permettent aussi de vérifier s'il y a eu de l'over ou under fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si vous voulez que votre nouveau modèle soit pris en compte par l'api, il faut le sauvegarder\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage de quelques résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédit sur les données du dataset de validation\n",
    "Y_pred = model.predict(X_val)\n",
    "\n",
    "# Récupère le chiffre avec la plus grande probabilité pour chaque prédiction\n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "print('2ème étape :',Y_pred[0], ' --> ', Y_pred_classes[0],'\\n')\n",
    "\n",
    "# Change Y_val de \"one hot vectors\" vers un chiffre unique\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "print('3ème étape :',Y_val[0],' --> ',Y_true[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelques erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_errors(errors_index, img_errors, pred_errors, obs_errors):\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            ax[row,col].axis('off')\n",
    "            n += 1\n",
    "    plt.subplots_adjust(hspace = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "get_random = [random.randrange(0, len(X_val_errors)) for i in range(6)]\n",
    "\n",
    "display_errors(get_random, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrice de confusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
